{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/ggiam/OneDrive/Documents/Projects/Word2Vec_Project/WordNet_Extraction/definitions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "definitions    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    \n",
    "    # Removing this temporarily as dictionary definitions with two words \n",
    "    # can be helpful in this context \n",
    "    \n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 1:\n",
    "        return ' '.join(txt)\n",
    "    \n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['definitions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 2.34 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_process=-1)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114674, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:12:17: collecting all words and their counts\n",
      "INFO - 16:12:17: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 16:12:17: PROGRESS: at sentence #10000, processed 78695 words and 76862 word types\n",
      "INFO - 16:12:17: PROGRESS: at sentence #20000, processed 142890 words and 129930 word types\n",
      "INFO - 16:12:17: PROGRESS: at sentence #30000, processed 210435 words and 178468 word types\n",
      "INFO - 16:12:17: PROGRESS: at sentence #40000, processed 275415 words and 219402 word types\n",
      "INFO - 16:12:17: PROGRESS: at sentence #50000, processed 339744 words and 259214 word types\n",
      "INFO - 16:12:17: PROGRESS: at sentence #60000, processed 410641 words and 304371 word types\n",
      "INFO - 16:12:18: PROGRESS: at sentence #70000, processed 482503 words and 345415 word types\n",
      "INFO - 16:12:18: PROGRESS: at sentence #80000, processed 542463 words and 378538 word types\n",
      "INFO - 16:12:18: PROGRESS: at sentence #90000, processed 631686 words and 416319 word types\n",
      "INFO - 16:12:18: PROGRESS: at sentence #100000, processed 703064 words and 456593 word types\n",
      "INFO - 16:12:18: PROGRESS: at sentence #110000, processed 765268 words and 496088 word types\n",
      "INFO - 16:12:18: collected 514949 token types (unigram + bigrams) from a corpus of 795315 words and 114674 sentences\n",
      "INFO - 16:12:18: merged Phrases<514949 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 16:12:18: Phrases lifecycle event {'msg': 'built Phrases<514949 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.77s', 'datetime': '2025-01-24T16:12:18.366937', 'gensim': '4.3.3', 'python': '3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "sent = [row.split() for row in df_clean['clean']]\n",
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:12:18: exporting phrases from Phrases<514949 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 16:12:19: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<265 phrases, min_count=30, threshold=10.0> from Phrases<514949 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.77s', 'datetime': '2025-01-24T16:12:19.148723', 'gensim': '4.3.3', 'python': '3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43042"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have',\n",
       " 'small',\n",
       " 'especially',\n",
       " 'relate',\n",
       " 'large',\n",
       " 'person',\n",
       " 'form',\n",
       " 'usually',\n",
       " 'act',\n",
       " 'manner']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:12:19: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2025-01-24T16:12:19.997371', 'gensim': '4.3.3', 'python': '3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:12:20: collecting all words and their counts\n",
      "INFO - 16:12:20: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 16:12:20: PROGRESS: at sentence #10000, processed 78351 words, keeping 14753 word types\n",
      "INFO - 16:12:20: PROGRESS: at sentence #20000, processed 141766 words, keeping 22220 word types\n",
      "INFO - 16:12:20: PROGRESS: at sentence #30000, processed 207945 words, keeping 26084 word types\n",
      "INFO - 16:12:20: PROGRESS: at sentence #40000, processed 270866 words, keeping 29118 word types\n",
      "INFO - 16:12:20: PROGRESS: at sentence #50000, processed 334435 words, keeping 31093 word types\n",
      "INFO - 16:12:20: PROGRESS: at sentence #60000, processed 404139 words, keeping 33120 word types\n",
      "INFO - 16:12:20: PROGRESS: at sentence #70000, processed 473470 words, keeping 34945 word types\n",
      "INFO - 16:12:20: PROGRESS: at sentence #80000, processed 531260 words, keeping 36716 word types\n",
      "INFO - 16:12:20: PROGRESS: at sentence #90000, processed 612509 words, keeping 39292 word types\n",
      "INFO - 16:12:20: PROGRESS: at sentence #100000, processed 682532 words, keeping 41085 word types\n",
      "INFO - 16:12:20: PROGRESS: at sentence #110000, processed 744406 words, keeping 42532 word types\n",
      "INFO - 16:12:20: collected 43042 word types from a corpus of 774361 raw words and 114674 sentences\n",
      "INFO - 16:12:20: Creating a fresh vocabulary\n",
      "INFO - 16:12:20: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 5927 unique words (13.77% of original 43042, drops 37115)', 'datetime': '2025-01-24T16:12:20.979447', 'gensim': '4.3.3', 'python': '3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 16:12:20: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 645322 word corpus (83.34% of original 774361, drops 129039)', 'datetime': '2025-01-24T16:12:20.981446', 'gensim': '4.3.3', 'python': '3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 16:12:21: deleting the raw counts dictionary of 43042 items\n",
      "INFO - 16:12:21: sample=6e-05 downsamples 1551 most-common words\n",
      "INFO - 16:12:21: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 405536.1505614936 word corpus (62.8%% of prior 645322)', 'datetime': '2025-01-24T16:12:21.091446', 'gensim': '4.3.3', 'python': '3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 16:12:21: estimated required memory for 5927 words and 300 dimensions: 17188300 bytes\n",
      "INFO - 16:12:21: resetting layer weights\n",
      "INFO - 16:12:21: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-01-24T16:12:21.153449', 'gensim': '4.3.3', 'python': '3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.02 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:12:21: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 5927 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2 shrink_windows=True', 'datetime': '2025-01-24T16:12:21.169613', 'gensim': '4.3.3', 'python': '3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "INFO - 16:12:22: EPOCH 0 - PROGRESS: at 57.00% examples, 219239 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:12:22: EPOCH 0: training on 774361 raw words (406104 effective words) took 1.4s, 282805 effective words/s\n",
      "INFO - 16:12:23: EPOCH 1 - PROGRESS: at 71.99% examples, 271095 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:12:23: EPOCH 1: training on 774361 raw words (405368 effective words) took 1.2s, 338517 effective words/s\n",
      "INFO - 16:12:24: EPOCH 2 - PROGRESS: at 67.92% examples, 269241 words/s, in_qsize 13, out_qsize 4\n",
      "INFO - 16:12:25: EPOCH 2: training on 774361 raw words (405569 effective words) took 1.1s, 354046 effective words/s\n",
      "INFO - 16:12:26: EPOCH 3 - PROGRESS: at 69.59% examples, 275608 words/s, in_qsize 10, out_qsize 3\n",
      "INFO - 16:12:26: EPOCH 3: training on 774361 raw words (406333 effective words) took 1.2s, 352755 effective words/s\n",
      "INFO - 16:12:27: EPOCH 4 - PROGRESS: at 74.16% examples, 292738 words/s, in_qsize 12, out_qsize 2\n",
      "INFO - 16:12:27: EPOCH 4: training on 774361 raw words (405562 effective words) took 1.1s, 371663 effective words/s\n",
      "INFO - 16:12:28: EPOCH 5 - PROGRESS: at 66.20% examples, 259297 words/s, in_qsize 10, out_qsize 4\n",
      "INFO - 16:12:28: EPOCH 5: training on 774361 raw words (405667 effective words) took 1.2s, 331541 effective words/s\n",
      "INFO - 16:12:29: EPOCH 6 - PROGRESS: at 57.00% examples, 220037 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 16:12:29: EPOCH 6: training on 774361 raw words (405282 effective words) took 1.4s, 298778 effective words/s\n",
      "INFO - 16:12:30: EPOCH 7 - PROGRESS: at 53.13% examples, 205485 words/s, in_qsize 7, out_qsize 8\n",
      "INFO - 16:12:31: EPOCH 7: training on 774361 raw words (406077 effective words) took 1.3s, 317879 effective words/s\n",
      "INFO - 16:12:32: EPOCH 8 - PROGRESS: at 83.20% examples, 336828 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:12:32: EPOCH 8: training on 774361 raw words (405732 effective words) took 1.1s, 374612 effective words/s\n",
      "INFO - 16:12:33: EPOCH 9 - PROGRESS: at 67.17% examples, 268825 words/s, in_qsize 8, out_qsize 6\n",
      "INFO - 16:12:33: EPOCH 9: training on 774361 raw words (405223 effective words) took 1.1s, 354884 effective words/s\n",
      "INFO - 16:12:34: EPOCH 10 - PROGRESS: at 67.83% examples, 268783 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 16:12:34: EPOCH 10: training on 774361 raw words (405643 effective words) took 1.2s, 347032 effective words/s\n",
      "INFO - 16:12:35: EPOCH 11 - PROGRESS: at 67.92% examples, 269110 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:12:35: EPOCH 11: training on 774361 raw words (405399 effective words) took 1.1s, 354042 effective words/s\n",
      "INFO - 16:12:36: EPOCH 12 - PROGRESS: at 82.12% examples, 330651 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:12:36: EPOCH 12: training on 774361 raw words (405629 effective words) took 1.1s, 383791 effective words/s\n",
      "INFO - 16:12:37: EPOCH 13 - PROGRESS: at 69.41% examples, 274133 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:12:38: EPOCH 13: training on 774361 raw words (405989 effective words) took 1.1s, 356897 effective words/s\n",
      "INFO - 16:12:39: EPOCH 14 - PROGRESS: at 60.76% examples, 239267 words/s, in_qsize 12, out_qsize 4\n",
      "INFO - 16:12:39: EPOCH 14: training on 774361 raw words (405386 effective words) took 1.4s, 297701 effective words/s\n",
      "INFO - 16:12:40: EPOCH 15 - PROGRESS: at 88.30% examples, 359376 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 16:12:40: EPOCH 15: training on 774361 raw words (405586 effective words) took 1.0s, 390509 effective words/s\n",
      "INFO - 16:12:41: EPOCH 16 - PROGRESS: at 86.86% examples, 353003 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 16:12:41: EPOCH 16: training on 774361 raw words (405574 effective words) took 1.0s, 389502 effective words/s\n",
      "INFO - 16:12:42: EPOCH 17 - PROGRESS: at 77.04% examples, 303558 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 16:12:42: EPOCH 17: training on 774361 raw words (405568 effective words) took 1.1s, 368537 effective words/s\n",
      "INFO - 16:12:43: EPOCH 18 - PROGRESS: at 74.03% examples, 295849 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:12:43: EPOCH 18: training on 774361 raw words (406001 effective words) took 1.1s, 373943 effective words/s\n",
      "INFO - 16:12:44: EPOCH 19 - PROGRESS: at 85.74% examples, 348035 words/s, in_qsize 11, out_qsize 0\n",
      "INFO - 16:12:44: EPOCH 19: training on 774361 raw words (405798 effective words) took 1.0s, 388035 effective words/s\n",
      "INFO - 16:12:45: EPOCH 20 - PROGRESS: at 69.50% examples, 271999 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:12:46: EPOCH 20: training on 774361 raw words (406210 effective words) took 1.1s, 356168 effective words/s\n",
      "INFO - 16:12:47: EPOCH 21 - PROGRESS: at 81.86% examples, 332973 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:12:47: EPOCH 21: training on 774361 raw words (405898 effective words) took 1.1s, 386072 effective words/s\n",
      "INFO - 16:12:48: EPOCH 22 - PROGRESS: at 89.57% examples, 364721 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 16:12:48: EPOCH 22: training on 774361 raw words (406069 effective words) took 1.0s, 396630 effective words/s\n",
      "INFO - 16:12:49: EPOCH 23 - PROGRESS: at 83.20% examples, 336008 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:12:49: EPOCH 23: training on 774361 raw words (405544 effective words) took 1.1s, 383780 effective words/s\n",
      "INFO - 16:12:50: EPOCH 24 - PROGRESS: at 79.50% examples, 313498 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:12:50: EPOCH 24: training on 774361 raw words (405116 effective words) took 1.1s, 372055 effective words/s\n",
      "INFO - 16:12:51: EPOCH 25 - PROGRESS: at 85.71% examples, 347267 words/s, in_qsize 11, out_qsize 0\n",
      "INFO - 16:12:51: EPOCH 25: training on 774361 raw words (405651 effective words) took 1.0s, 388758 effective words/s\n",
      "INFO - 16:12:52: EPOCH 26 - PROGRESS: at 77.34% examples, 312013 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:12:52: EPOCH 26: training on 774361 raw words (405584 effective words) took 1.1s, 377737 effective words/s\n",
      "INFO - 16:12:53: EPOCH 27 - PROGRESS: at 83.20% examples, 335794 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:12:53: EPOCH 27: training on 774361 raw words (405403 effective words) took 1.1s, 384886 effective words/s\n",
      "INFO - 16:12:54: EPOCH 28 - PROGRESS: at 84.48% examples, 343240 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 16:12:54: EPOCH 28: training on 774361 raw words (405384 effective words) took 1.0s, 388355 effective words/s\n",
      "INFO - 16:12:55: EPOCH 29 - PROGRESS: at 88.13% examples, 351496 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 16:12:55: EPOCH 29: training on 774361 raw words (405452 effective words) took 1.1s, 384844 effective words/s\n",
      "INFO - 16:12:55: Word2Vec lifecycle event {'msg': 'training on 23230830 raw words (12169801 effective words) took 34.6s, 351329 effective words/s', 'datetime': '2025-01-24T16:12:55.810076', 'gensim': '4.3.3', 'python': '3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.58 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('steel', 0.6740243434906006),\n",
       " ('perforate', 0.6680794954299927),\n",
       " ('tin', 0.6581662893295288),\n",
       " ('alloy', 0.6540107131004333),\n",
       " ('hammer', 0.6454702019691467),\n",
       " ('quartz', 0.6265373229980469),\n",
       " ('zinc', 0.6252149939537048),\n",
       " ('molten', 0.6223427057266235),\n",
       " ('iron', 0.6208430528640747),\n",
       " ('oxide', 0.6021556854248047)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"metal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 16:12:55: vectors for words {'arian'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'woman'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['man', 'woman', 'arian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have',\n",
       " 'small',\n",
       " 'especially',\n",
       " 'relate',\n",
       " 'large',\n",
       " 'person',\n",
       " 'form',\n",
       " 'usually',\n",
       " 'act',\n",
       " 'manner',\n",
       " 'cause',\n",
       " 'plant',\n",
       " 'leave',\n",
       " 'genus',\n",
       " 'state',\n",
       " 'united_states',\n",
       " 'time',\n",
       " 'water',\n",
       " 'body',\n",
       " 'long',\n",
       " 'people',\n",
       " 'work',\n",
       " 'flower',\n",
       " 'family',\n",
       " 'tree',\n",
       " \"'\",\n",
       " 'consist',\n",
       " 'place',\n",
       " 'use',\n",
       " 'group',\n",
       " 'resemble',\n",
       " 'order',\n",
       " 'contain',\n",
       " 'give',\n",
       " 'produce',\n",
       " 'shape',\n",
       " 'light',\n",
       " 'high',\n",
       " 'like',\n",
       " 'play',\n",
       " 'hold',\n",
       " 'member',\n",
       " 'system',\n",
       " 'white',\n",
       " 'good',\n",
       " 'child',\n",
       " 'quality',\n",
       " 'bear',\n",
       " 'point',\n",
       " 'man',\n",
       " 'line',\n",
       " 'area',\n",
       " 'surface',\n",
       " 'head',\n",
       " 'new',\n",
       " 'animal',\n",
       " 'low',\n",
       " 'language',\n",
       " 'find',\n",
       " 'result',\n",
       " 'great',\n",
       " 'color',\n",
       " 'end',\n",
       " 'fruit',\n",
       " 'cover',\n",
       " 'sound',\n",
       " 'take',\n",
       " 'law',\n",
       " 'lack',\n",
       " 'red',\n",
       " 'characterize',\n",
       " 'grow',\n",
       " 'include',\n",
       " 'force',\n",
       " 'common',\n",
       " 'position',\n",
       " 'change',\n",
       " 'woman',\n",
       " 'short',\n",
       " 'food',\n",
       " 'provide',\n",
       " 'write',\n",
       " 'make',\n",
       " 'process',\n",
       " 'word',\n",
       " 'branch',\n",
       " 'life',\n",
       " 'number',\n",
       " 'set',\n",
       " 'mark',\n",
       " 'base',\n",
       " 'city',\n",
       " 'property',\n",
       " 'action',\n",
       " 'money',\n",
       " 'live',\n",
       " 'right',\n",
       " 'day',\n",
       " 'serve',\n",
       " 'disease',\n",
       " 'power',\n",
       " 'river',\n",
       " 'herb',\n",
       " 'country',\n",
       " 'control',\n",
       " 'occur',\n",
       " 'way',\n",
       " 'cut',\n",
       " 'air',\n",
       " 'region',\n",
       " 'condition',\n",
       " 'fish',\n",
       " 'substance',\n",
       " 'cell',\n",
       " 'central',\n",
       " 'blood',\n",
       " 'year',\n",
       " 'sea',\n",
       " 'black',\n",
       " 'study',\n",
       " 'northern',\n",
       " 'come',\n",
       " 'government',\n",
       " 'certain',\n",
       " 'hand',\n",
       " 'material',\n",
       " 'unit',\n",
       " 'thing',\n",
       " 'involve',\n",
       " 'wood',\n",
       " 'states',\n",
       " 'face',\n",
       " 'english',\n",
       " 'open',\n",
       " 'support',\n",
       " 'carry',\n",
       " 'equal',\n",
       " 'eye',\n",
       " 'particular',\n",
       " 'old',\n",
       " 'yellow',\n",
       " 'type_genus',\n",
       " 'wear',\n",
       " 'skin',\n",
       " 'book',\n",
       " 'hard',\n",
       " 'liquid',\n",
       " 'turn',\n",
       " 'etc',\n",
       " 'bird',\n",
       " 'piece',\n",
       " 'foot',\n",
       " 'activity',\n",
       " 'american',\n",
       " 'degree',\n",
       " 'value',\n",
       " 'know',\n",
       " 'capable',\n",
       " 'military',\n",
       " 'object',\n",
       " 'house',\n",
       " 'design',\n",
       " 'characteristic',\n",
       " 'island',\n",
       " 'human',\n",
       " 'metal',\n",
       " 'run',\n",
       " 'car',\n",
       " 'ship',\n",
       " 'lead',\n",
       " 'horse',\n",
       " 'land',\n",
       " 'part',\n",
       " 'green',\n",
       " 'southern',\n",
       " 'tropical',\n",
       " 'structure',\n",
       " 'move',\n",
       " 'game',\n",
       " 'european',\n",
       " 'hair',\n",
       " 'remove',\n",
       " 'french',\n",
       " 'speak',\n",
       " 'dry',\n",
       " 'pass',\n",
       " 'treat',\n",
       " 'public',\n",
       " 'close',\n",
       " 'shrub',\n",
       " 'strong',\n",
       " 'western',\n",
       " 'free',\n",
       " 'ground',\n",
       " 'war',\n",
       " 'follow',\n",
       " 'music',\n",
       " 'note',\n",
       " 'near',\n",
       " 'show',\n",
       " 'develop',\n",
       " 'seed',\n",
       " 'center',\n",
       " 'fire',\n",
       " 'town',\n",
       " 'flow',\n",
       " 'event',\n",
       " 'measure',\n",
       " 'early',\n",
       " 'muscle',\n",
       " 'term',\n",
       " 'different',\n",
       " 'ball',\n",
       " 'information',\n",
       " 'having',\n",
       " 'drug',\n",
       " 'oil',\n",
       " 'church',\n",
       " 'source',\n",
       " 'young',\n",
       " 'earth',\n",
       " 'perform',\n",
       " 'europe',\n",
       " 'god',\n",
       " 'express',\n",
       " 'mean',\n",
       " 'british',\n",
       " 'effect',\n",
       " 'room',\n",
       " 'perennial',\n",
       " 'charge',\n",
       " 'america',\n",
       " 'building',\n",
       " 'social',\n",
       " 'away',\n",
       " 'bring',\n",
       " 'tissue',\n",
       " 'create',\n",
       " 'wall',\n",
       " 'single',\n",
       " 'movement',\n",
       " 'fall',\n",
       " 'feeling',\n",
       " 'computer',\n",
       " 'thin',\n",
       " 'function',\n",
       " 'break',\n",
       " 'stem',\n",
       " 'draw',\n",
       " 'feel',\n",
       " 'fungus',\n",
       " 'look',\n",
       " 'similar',\n",
       " 'behavior',\n",
       " 'go',\n",
       " 'party',\n",
       " 'pay',\n",
       " 'direction',\n",
       " 'consider',\n",
       " 'increase',\n",
       " 'device',\n",
       " 'class',\n",
       " 'business',\n",
       " 'acid',\n",
       " 'world',\n",
       " 'kind',\n",
       " 'space',\n",
       " 'brown',\n",
       " 'blue',\n",
       " 'edible',\n",
       " 'ancient',\n",
       " 'subject',\n",
       " 'soft',\n",
       " 'north_american',\n",
       " 'purpose',\n",
       " 'eg',\n",
       " 'heat',\n",
       " 'arm',\n",
       " 'deep',\n",
       " 'dark',\n",
       " 'gas',\n",
       " 'paper',\n",
       " 'school',\n",
       " 'heart',\n",
       " 'practice',\n",
       " 'important',\n",
       " 'north_america',\n",
       " 'interest',\n",
       " 'student',\n",
       " 'heavy',\n",
       " 'physical',\n",
       " 'matter',\n",
       " 'rock',\n",
       " 'eastern',\n",
       " 'locate',\n",
       " 'bone',\n",
       " 'rule',\n",
       " 'organ',\n",
       " 'root',\n",
       " 'service',\n",
       " 'try',\n",
       " 'separate',\n",
       " 'sell',\n",
       " 'wind',\n",
       " 'character',\n",
       " 'trade',\n",
       " 'north',\n",
       " 'company',\n",
       " 'little',\n",
       " 'compound',\n",
       " 'need',\n",
       " 'natural',\n",
       " 'period',\n",
       " 'build',\n",
       " 'concern',\n",
       " 'home',\n",
       " 'leg',\n",
       " 'political',\n",
       " 'fill',\n",
       " 'art',\n",
       " 'idea',\n",
       " 'eat',\n",
       " 'player',\n",
       " 'energy',\n",
       " 'court',\n",
       " 'africa',\n",
       " 'obtain',\n",
       " 'mountain',\n",
       " 'cultivate',\n",
       " 'general',\n",
       " 'record',\n",
       " 'attack',\n",
       " 'flat',\n",
       " 'mind',\n",
       " 'deal',\n",
       " 'white_flower',\n",
       " 'price',\n",
       " 'second',\n",
       " 'coat',\n",
       " 'raise',\n",
       " 'south',\n",
       " 'travel',\n",
       " 'male',\n",
       " 'religious',\n",
       " 'specie',\n",
       " 'letter',\n",
       " 'old_world',\n",
       " 'field',\n",
       " 'present',\n",
       " 'fern',\n",
       " 'element',\n",
       " 'intend',\n",
       " 'plane',\n",
       " 'indicate',\n",
       " 'case',\n",
       " 'size',\n",
       " 'instrument',\n",
       " 'theory',\n",
       " 'development',\n",
       " 'vein',\n",
       " 'speech',\n",
       " 'yield',\n",
       " 'mass',\n",
       " 'insect',\n",
       " 'variety',\n",
       " 'native',\n",
       " 'spread',\n",
       " 'england',\n",
       " 'extend',\n",
       " 'dress',\n",
       " 'drive',\n",
       " 'receive',\n",
       " 'official',\n",
       " 'quantity',\n",
       " 'capital',\n",
       " 'pressure',\n",
       " 'death',\n",
       " 'tail',\n",
       " 'train',\n",
       " 'wing',\n",
       " 'age',\n",
       " 'stage',\n",
       " 'office',\n",
       " 'plan',\n",
       " 'determine',\n",
       " 'style',\n",
       " 'grass',\n",
       " 'get',\n",
       " 'sun',\n",
       " 'nature',\n",
       " 'motion',\n",
       " 'king',\n",
       " 'reduce',\n",
       " 'belong',\n",
       " 'language_speak',\n",
       " 'require',\n",
       " 'edge',\n",
       " 'california',\n",
       " 'meat',\n",
       " 'scale',\n",
       " 'keep',\n",
       " 'tall',\n",
       " 'relate_characteristic',\n",
       " 'cluster',\n",
       " 'grey',\n",
       " 'associate',\n",
       " 'establish',\n",
       " 'wine',\n",
       " 'affect',\n",
       " 'film',\n",
       " 'relation',\n",
       " 'plant_genus',\n",
       " 'walk',\n",
       " 'feed',\n",
       " 'lose',\n",
       " 'supply',\n",
       " 'fabric',\n",
       " 'agent',\n",
       " 'attach',\n",
       " 'th_century',\n",
       " 'major',\n",
       " 'asia',\n",
       " 'apply',\n",
       " 'chemical',\n",
       " 'salt',\n",
       " 'loss',\n",
       " 'yellow_flower',\n",
       " 'store',\n",
       " 'dog',\n",
       " 'stand',\n",
       " 'night',\n",
       " 'self',\n",
       " 'thick',\n",
       " 'think',\n",
       " 'type',\n",
       " 'pain',\n",
       " 'direct',\n",
       " 'rise',\n",
       " 'principle',\n",
       " 'operation',\n",
       " 'connect',\n",
       " 'hot',\n",
       " 'series',\n",
       " 'fine',\n",
       " 'divide',\n",
       " 'organism',\n",
       " 'narrow',\n",
       " 'sense',\n",
       " 'view',\n",
       " 'add',\n",
       " 'sharp',\n",
       " 'derive',\n",
       " 'lie',\n",
       " 'experience',\n",
       " 'job',\n",
       " 'prevent',\n",
       " 'individual',\n",
       " 'expression',\n",
       " 'purple',\n",
       " 'specific',\n",
       " 'influence',\n",
       " 'star',\n",
       " 'believe',\n",
       " 'care',\n",
       " 'level',\n",
       " 'represent',\n",
       " 'authority',\n",
       " 'female',\n",
       " 'legal',\n",
       " 'german',\n",
       " 'voice',\n",
       " 'chiefly',\n",
       " 'stop',\n",
       " 'organization',\n",
       " 'india',\n",
       " 'program',\n",
       " 'clear',\n",
       " 'cold',\n",
       " 'tooth',\n",
       " 'product',\n",
       " 'disorder',\n",
       " 'distance',\n",
       " 'fact',\n",
       " 'protect',\n",
       " 'return',\n",
       " 'fly',\n",
       " 'course',\n",
       " 'team',\n",
       " 'call',\n",
       " 'west',\n",
       " 'bc',\n",
       " 'table',\n",
       " 'iron',\n",
       " 'normal',\n",
       " 'problem',\n",
       " 'annual',\n",
       " 'dance',\n",
       " 'soil',\n",
       " 'fix',\n",
       " 'drink',\n",
       " 'battle',\n",
       " 'france',\n",
       " 'bright',\n",
       " 'numerous',\n",
       " 'kill',\n",
       " 'machine',\n",
       " 'slender',\n",
       " 'true',\n",
       " 'mouth',\n",
       " 'situation',\n",
       " 'start',\n",
       " 'smooth',\n",
       " 'upper',\n",
       " 'hit',\n",
       " 'strike',\n",
       " 'stone',\n",
       " 'egg',\n",
       " 'love',\n",
       " 'bad',\n",
       " 'issue',\n",
       " 'evergreen',\n",
       " 'highly',\n",
       " 'print',\n",
       " 'writer',\n",
       " 'wave',\n",
       " 'east',\n",
       " 'mental',\n",
       " 'breed',\n",
       " 'layer',\n",
       " 'medicine',\n",
       " 'cook',\n",
       " 'solution',\n",
       " 'pair',\n",
       " 'stock',\n",
       " 'respect',\n",
       " 'sweet',\n",
       " 'throw',\n",
       " 'australia',\n",
       " 'story',\n",
       " 'help',\n",
       " 'particle',\n",
       " 'mother',\n",
       " 'race',\n",
       " 'temperature',\n",
       " 'worker',\n",
       " 'warm',\n",
       " 'catch',\n",
       " 'question',\n",
       " 'sexual',\n",
       " 'president',\n",
       " 'china',\n",
       " 'performance',\n",
       " 'signal',\n",
       " 'operate',\n",
       " 'lake',\n",
       " 'appear',\n",
       " 'plate',\n",
       " 'appearance',\n",
       " 'rank',\n",
       " 'handle',\n",
       " 'special',\n",
       " 'regard',\n",
       " 'wife',\n",
       " 'treatment',\n",
       " 'cross',\n",
       " 'road',\n",
       " 'wild',\n",
       " 'range',\n",
       " 'sugar',\n",
       " 'vehicle',\n",
       " 'glass',\n",
       " 'win',\n",
       " 'blow',\n",
       " 'complete',\n",
       " 'responsible',\n",
       " 'african',\n",
       " 'coast',\n",
       " 'accept',\n",
       " 'knowledge',\n",
       " 'aromatic',\n",
       " 'round',\n",
       " 'grain',\n",
       " 'nerve',\n",
       " 'rate',\n",
       " 'see',\n",
       " 'card',\n",
       " 'northeastern',\n",
       " 'pull',\n",
       " 'conduct',\n",
       " 'army',\n",
       " 'erect',\n",
       " 'fit',\n",
       " 'infection',\n",
       " 'standard',\n",
       " 'joint',\n",
       " 'enemy',\n",
       " 'desire',\n",
       " 'begin',\n",
       " 'burn',\n",
       " 'growth',\n",
       " 'neck',\n",
       " 'fluid',\n",
       " 'paint',\n",
       " 'bank',\n",
       " 'current',\n",
       " 'attention',\n",
       " 'mammal',\n",
       " 'stalk',\n",
       " 'fast',\n",
       " 'exist',\n",
       " 'son',\n",
       " 'defeat',\n",
       " 'site',\n",
       " 'passage',\n",
       " 'ear',\n",
       " 'tend',\n",
       " 'hole',\n",
       " 'pattern',\n",
       " 'door',\n",
       " 'active',\n",
       " 'shell',\n",
       " 'allow',\n",
       " 'test',\n",
       " 'effort',\n",
       " 'image',\n",
       " 'prepare',\n",
       " 'mexico',\n",
       " 'sport',\n",
       " 'arrange',\n",
       " 'orange',\n",
       " 'solid',\n",
       " 'street',\n",
       " 'tube',\n",
       " 'belief',\n",
       " 'destroy',\n",
       " 'wheel',\n",
       " 'personal',\n",
       " 'republic',\n",
       " 'milk',\n",
       " 'board',\n",
       " 'marine',\n",
       " 'father',\n",
       " 'transmit',\n",
       " 'able',\n",
       " 'officer',\n",
       " 'accord',\n",
       " 'native_inhabitant',\n",
       " 'hour',\n",
       " 'enter',\n",
       " 'die',\n",
       " 'month',\n",
       " 'policy',\n",
       " 'fight',\n",
       " 'remain',\n",
       " 'weight',\n",
       " 'main',\n",
       " 'pertain',\n",
       " 'engine',\n",
       " 'roman',\n",
       " 'oppose',\n",
       " 'wide',\n",
       " 'northwestern',\n",
       " 'society',\n",
       " 'bed',\n",
       " 'south_america',\n",
       " 'sign',\n",
       " 'economic',\n",
       " 'novel',\n",
       " 'parent',\n",
       " 'inflammation',\n",
       " 'duty',\n",
       " 'half',\n",
       " 'late',\n",
       " 'seat',\n",
       " 'flesh',\n",
       " 'vessel',\n",
       " 'outside',\n",
       " 'doctrine',\n",
       " 'join',\n",
       " 'display',\n",
       " 'share',\n",
       " 'exercise',\n",
       " 'engage',\n",
       " 'pink',\n",
       " 'leader',\n",
       " 'read',\n",
       " 'southwestern',\n",
       " 'southeastern',\n",
       " 'australian',\n",
       " 'compose',\n",
       " 'release',\n",
       " 'curve',\n",
       " 'buy',\n",
       " 'greek',\n",
       " 'broad',\n",
       " 'agency',\n",
       " 'side',\n",
       " 'leaf',\n",
       " 'relatively',\n",
       " 'ornamental',\n",
       " 'contract',\n",
       " 'taste',\n",
       " 'equipment',\n",
       " 'ability',\n",
       " 'figure',\n",
       " 'rich',\n",
       " 'want',\n",
       " 'widely',\n",
       " 'send',\n",
       " 'spot',\n",
       " 'brain',\n",
       " 'easily',\n",
       " 'bark',\n",
       " 'dead',\n",
       " 'extreme',\n",
       " 'limit',\n",
       " 'sudden',\n",
       " 'nation',\n",
       " 'cap',\n",
       " 'bar',\n",
       " 'project',\n",
       " 'flavor',\n",
       " 'bill',\n",
       " 'talk',\n",
       " 'reach',\n",
       " 'combine',\n",
       " 'patient',\n",
       " 'ice',\n",
       " 'science',\n",
       " 'enclose',\n",
       " 'method',\n",
       " 'evidence',\n",
       " 'production',\n",
       " 'weather',\n",
       " 'division',\n",
       " 'security',\n",
       " 'square',\n",
       " 'length',\n",
       " 'maintain',\n",
       " 'showy',\n",
       " 'abnormal',\n",
       " 'popular',\n",
       " 'window',\n",
       " 'boat',\n",
       " 'surround',\n",
       " 'clothe',\n",
       " 'basis',\n",
       " 'block',\n",
       " 'specify',\n",
       " 'hang',\n",
       " 'opposite',\n",
       " 'medical',\n",
       " 'convert',\n",
       " 'hear',\n",
       " 'account',\n",
       " 'suitable',\n",
       " 'slow',\n",
       " 'foliage',\n",
       " 'formal',\n",
       " 'aircraft',\n",
       " 'department',\n",
       " 'datum',\n",
       " 'university',\n",
       " 'simple',\n",
       " 'advance',\n",
       " 'excessive',\n",
       " 'border',\n",
       " 'meaning',\n",
       " 'opinion',\n",
       " 'musical',\n",
       " 'picture',\n",
       " 'rope',\n",
       " 'cloth',\n",
       " 'union',\n",
       " 'far',\n",
       " 'ii',\n",
       " 'strip',\n",
       " 'roll',\n",
       " 'clothing',\n",
       " 'middle',\n",
       " 'fiber',\n",
       " 'gain',\n",
       " 'clean',\n",
       " 'statement',\n",
       " 'italian',\n",
       " 'say',\n",
       " 'let',\n",
       " 'meet',\n",
       " 'offer',\n",
       " 'bacteria',\n",
       " 'sleep',\n",
       " 'well',\n",
       " 'emotion',\n",
       " 'reason',\n",
       " 'ring',\n",
       " 'forward',\n",
       " 'gun',\n",
       " 'real',\n",
       " 'gland',\n",
       " 'news',\n",
       " 'document',\n",
       " 'baseball',\n",
       " 'soldier',\n",
       " 'noise',\n",
       " 'fragrant',\n",
       " 'spirit',\n",
       " 'strength',\n",
       " 'damage',\n",
       " 'modern',\n",
       " 'writing',\n",
       " 'weapon',\n",
       " 'introduce',\n",
       " 'completely',\n",
       " 'severe',\n",
       " 'commercial',\n",
       " 'snake',\n",
       " 'angle',\n",
       " 'opening',\n",
       " 'shoe',\n",
       " 'finger',\n",
       " 'basic_unit',\n",
       " 'atlantic',\n",
       " 'floor',\n",
       " 'bind',\n",
       " 'name',\n",
       " 'drop',\n",
       " 'step',\n",
       " 'available',\n",
       " 'distinguish',\n",
       " 'regular',\n",
       " 'beat',\n",
       " 'list',\n",
       " 'feature',\n",
       " 'possible',\n",
       " 'expect',\n",
       " 'section',\n",
       " 'christian',\n",
       " 'communication',\n",
       " 'outer',\n",
       " 'tone',\n",
       " 'combination',\n",
       " 'extremely',\n",
       " 'fat',\n",
       " 'future',\n",
       " 'payment',\n",
       " 'suffer',\n",
       " 'touch',\n",
       " 'hope',\n",
       " 'extinct',\n",
       " 'reaction',\n",
       " 'raceme',\n",
       " 'tie',\n",
       " 'culture',\n",
       " 'claim',\n",
       " 'electric',\n",
       " 'ride',\n",
       " 'formation',\n",
       " 'boy',\n",
       " 'pitch',\n",
       " 'tell',\n",
       " 'fresh',\n",
       " 'greek_mythology',\n",
       " 'forest',\n",
       " 'friend',\n",
       " 'radio',\n",
       " 'police',\n",
       " 'difficult',\n",
       " 'check',\n",
       " 'mixture',\n",
       " 'market',\n",
       " 'steel',\n",
       " 'garment',\n",
       " 'eurasian',\n",
       " 'aquatic',\n",
       " 'sale',\n",
       " 'gold',\n",
       " 'discover',\n",
       " 'thought',\n",
       " 'past',\n",
       " 'world_war',\n",
       " 'spring',\n",
       " 'health',\n",
       " 'mineral',\n",
       " 'component',\n",
       " 'eastern_north',\n",
       " 'artery',\n",
       " 'skill',\n",
       " 'atom',\n",
       " 'pale',\n",
       " 'exchange',\n",
       " 'fail',\n",
       " 'season',\n",
       " 'vegetable',\n",
       " 'origin',\n",
       " 'title',\n",
       " 'failure',\n",
       " 'poisonous',\n",
       " 'classification',\n",
       " 'response',\n",
       " 'argument',\n",
       " 'birth',\n",
       " 'inside',\n",
       " 'marriage',\n",
       " 'collection',\n",
       " 'shoulder',\n",
       " 'fear',\n",
       " 'cavity',\n",
       " 'rain',\n",
       " 'japan',\n",
       " 'mediterranean',\n",
       " 'deposit',\n",
       " 'comprise',\n",
       " 'minute',\n",
       " 'desert',\n",
       " 'text',\n",
       " 'juice',\n",
       " 'crown',\n",
       " 'bomb',\n",
       " 'medium',\n",
       " 'financial',\n",
       " 'box',\n",
       " 'ray',\n",
       " 'troop',\n",
       " 'relative',\n",
       " 'tool',\n",
       " 'capacity',\n",
       " 'bread',\n",
       " 'teacher',\n",
       " 'press',\n",
       " 'remember',\n",
       " 'terrestrial',\n",
       " 'role',\n",
       " 'big',\n",
       " 'loose',\n",
       " 'band',\n",
       " 'chief',\n",
       " 'pacific',\n",
       " 'later',\n",
       " 'sex',\n",
       " 'adult',\n",
       " 'electronic',\n",
       " 'orchid',\n",
       " 'judge',\n",
       " 'roof',\n",
       " 'pipe',\n",
       " 'stick',\n",
       " 'girl',\n",
       " 'nuclear',\n",
       " 'location',\n",
       " 'tropical_american',\n",
       " 'demand',\n",
       " 'tip',\n",
       " 'perennial_herb',\n",
       " 'local',\n",
       " 'actor',\n",
       " 'flight',\n",
       " 'sit',\n",
       " 'religion',\n",
       " 'history',\n",
       " 'husband',\n",
       " 'container',\n",
       " 'report',\n",
       " 'vine',\n",
       " 'memory',\n",
       " 'complex',\n",
       " 'parallel',\n",
       " 'institution',\n",
       " 'continue',\n",
       " 'loud',\n",
       " 'living',\n",
       " 'protein',\n",
       " 'garden',\n",
       " 'research',\n",
       " 's',\n",
       " 'presence',\n",
       " 'spike',\n",
       " 'agreement',\n",
       " 'electrical',\n",
       " 'found',\n",
       " 'meeting',\n",
       " 'movie',\n",
       " 'vote',\n",
       " 'sauce',\n",
       " 'answer',\n",
       " 'deliver',\n",
       " 'intense',\n",
       " 'finish',\n",
       " 'alcohol',\n",
       " 'arrangement',\n",
       " 'happen',\n",
       " 'smell',\n",
       " 'grant',\n",
       " 'federal',\n",
       " 'advocate',\n",
       " 'organize',\n",
       " 'date',\n",
       " 'constitute',\n",
       " 'widely_distribute',\n",
       " 'port',\n",
       " 'famous',\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:15:57: storing 5927x300 projection weights into dictionary_model.bin\n"
     ]
    }
   ],
   "source": [
    "w2v_model.wv.save_word2vec_format(\"dictionary_model.bin\", binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
